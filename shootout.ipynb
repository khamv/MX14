{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"title\">Nonparametric Method Shootout</p>\n",
    "\n",
    "I hope you're convinced that Student-t intervals don't necessarily have true coverage levels close to their nominal coverage levels, even for large sample sizes.\n",
    "\n",
    "Moreover, there are a variety of conservative nonparametric methods that can be used for populations with one-sided or two-sided bounds to produce one-sided or two-sided confidence intervals guaranteed to have coverage probabilities at least as large as their nominal confidence level.\n",
    "\n",
    "Which is best?\n",
    "\n",
    "If the population really consists of only two values, it is impossible to improve on exact Binomial intervals for samples drawn with replacement or Hypergeometric intervals for sampling without replacement (for one-sided bounds; for two-sided bounds, there is no unique \"best\" choice).\n",
    "\n",
    "For more general populations, your mileage may vary.\n",
    "\n",
    "Let's do some experiments to compare them. None is best in every situation. Relative performance depends on the population distribution and on sample sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Penny Sampling, Kaplan-Wald, Hoeffding, and MDKW\n",
    "Let's compare the principal methods we've developed, using simulations from a broader variety of populations. We will skip the thresholded binomial, Chebychev's inequality, and Markov's inequality: they are dominated by other methods.\n",
    "\n",
    "Some of the methods (Hoeffding, Penny Sampling) require upper and lower population bounds. When they are applicable, we might expect them to do better than methods that require only one-sided population bounds (MDKW, Kaplan-Wald), since they use more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the first cell with code: set up the Python environment\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "from scipy.stats import binom\n",
    "import scipy.optimize\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binoLowerCL(n, x, cl = 0.975, inc=0.000001, p = None):\n",
    "    \"Lower confidence level cl confidence interval for Binomial p, for x successes in n trials\"\n",
    "    if p is None:\n",
    "            p = float(x)/float(n)\n",
    "    lo = 0.0\n",
    "    if (x > 0):\n",
    "            f = lambda q: cl - scipy.stats.binom.cdf(x-1, n, q)\n",
    "            lo = sp.optimize.brentq(f, 0.0, p, xtol=inc)\n",
    "    return lo\n",
    "\n",
    "def binoUpperCL(n, x, cl = 0.975, inc=0.000001, p = None):\n",
    "    \"Upper confidence level cl confidence interval for Binomial p, for x successes in n trials\"\n",
    "    if p is None:\n",
    "            p = float(x)/float(n)\n",
    "    hi = 1.0\n",
    "    if (x < n):\n",
    "            f = lambda q: scipy.stats.binom.cdf(x, n, q) - (1-cl)\n",
    "            hi = sp.optimize.brentq(f, p, 1.0, xtol=inc) \n",
    "    return hi\n",
    "\n",
    "def ecdf(x):\n",
    "    '''\n",
    "       calculates the empirical cdf of data x\n",
    "       returns the unique values of x in ascending order and the cumulative probabity at those values\n",
    "       NOTE: This is not an efficient algorithm: it is O(n^2), where n is the length of x. \n",
    "       A better algorithm would rely on the Collections package or something similar and could work\n",
    "       in O(n log n)\n",
    "    '''\n",
    "    theVals = sorted(np.unique(x))\n",
    "    theProbs = np.array([sum(x <= v) for v in theVals])/float(len(x))\n",
    "    if (theVals[0] > 0.0):\n",
    "        theVals = np.append(0., theVals)\n",
    "        theProbs = np.append(0., theProbs)\n",
    "    return theVals, theProbs    \n",
    "    \n",
    "def ksLowerMean(x, c):\n",
    "    '''\n",
    "       lower confidence bound for the mean of a nonnegative population\n",
    "       x is an iid sample with replacement from the population\n",
    "       c is the Massart constant for the desired coverage\n",
    "    '''\n",
    "    # find the ecdf\n",
    "    vals, probs = ecdf(x)\n",
    "    probs = np.fmin(probs+c, 1)   # This is G^-\n",
    "    gProbs = np.diff(np.append([0.0], probs))  # pre-pend a 0 so that diff does the right thing; \n",
    "                                               # gProbs is the vector of masses\n",
    "    return (vals*gProbs).sum()\n",
    "\n",
    "def kaplanWaldLowerCI(x, cl = 0.95, gamma = 0.99, xtol=1.e-12, logf=True):\n",
    "    '''\n",
    "       Calculates the Kaplan-Wald lower 1-alpha confidence bound for the mean of a nonnegative random\n",
    "       variable.\n",
    "    '''\n",
    "    alpha = 1.0-cl\n",
    "    if any(x < 0):\n",
    "        raise ValueError('Data x must be nonnegative.')\n",
    "    elif all(x <= 0):\n",
    "        lo = 0.0\n",
    "    else:\n",
    "        if logf:\n",
    "            f = lambda t: (np.max(np.cumsum(np.log(gamma*x/t + 1 - gamma))) + np.log(alpha))\n",
    "        else:\n",
    "            f = lambda t: (np.max(np.cumprod(gamma*x/t + 1 - gamma)) - 1/alpha)\n",
    "        xm = np.mean(x)\n",
    "        if f(xtol)*f(xm) > 0.0:\n",
    "            lo = 0.0\n",
    "        else:\n",
    "            lo = sp.optimize.brentq(f, xtol, np.mean(x), xtol=xtol) \n",
    "    return lo\n",
    "\n",
    "def pennySampleReplacement(weights, n):\n",
    "    '''\n",
    "       Weighted random sample of size n drawn with replacement.\n",
    "       Returns indices of the selected items, the \"remainder pennies,\"\n",
    "       and the raw uniform values used to select the sample\n",
    "    '''\n",
    "    if any(weights < 0):\n",
    "        print 'negative weight in weightedRandomSample'\n",
    "        return float('NaN')\n",
    "    else:\n",
    "        totWt = np.sum(weights, dtype=float)\n",
    "        wc = np.cumsum(weights, dtype=float)/totWt  # ensure weights sum to 1\n",
    "        theSam = np.random.random_sample((n,))\n",
    "        inx = np.array(wc).searchsorted(theSam)\n",
    "        penny = [(wc[inx[i]]-theSam[i])*totWt for i in range(n)]\n",
    "        return inx, penny, theSam\n",
    "\n",
    "def pennyBinomialLowerBound(x, inx, pennies, cl=0.95):\n",
    "    '''\n",
    "       Penny sampling lower (one-sided) 1-alpha confidence bound on the mean, for sampling with replacement.\n",
    "       x is the vector of observed values\n",
    "       pennies is the vector of _which_ \"penny\" in each sampled item is to be adjudicated as \"good\" or \"bad\"\n",
    "       The first x_j pennies in item j are deemed \"good,\" the remaining (u_j - x_j) are \"bad.\"\n",
    "       Returns the lower bound and the number of \"good\" pennies in the sample.\n",
    "    '''\n",
    "    s = sum([pennies[i] <= x[inx[i]] for i in range(len(pennies))])\n",
    "    n = len(inx)\n",
    "    return binoLowerCL(n, s, cl=cl), s\n",
    "\n",
    "def pennyBinomialBounds(x, inx, pennies, cl=0.95):\n",
    "    '''\n",
    "       Penny sampling 2-sided confidence interval for the mean, for sampling with replacement.\n",
    "       x is the vector of observed values\n",
    "       pennies is the vector of _which_ \"penny\" in each sampled item is to be adjudicated as \"good\" or \"bad\"\n",
    "       The first x_j pennies in item j are deemed \"good,\" the remaining (u_j - x_j) are \"bad.\"\n",
    "       Returns the lower bound, the upper bound and the number of \"good\" pennies in the sample.\n",
    "    '''\n",
    "    s = sum([pennies[i] <= x[inx[i]] for i in range(len(pennies))])\n",
    "    n = len(inx)\n",
    "    return binoLowerCL(n, s, cl=1-(1-cl)/2), binoUpperCL(n, s, cl=1-(1-cl)/2), s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare lower confidence bounds using truncated Hoeffding, MDKW, Kaplan-Wald, and Continuous Penny Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Simulated coverage probability and expected lengths of one-sided nonparametric confidence intervals mixture of U[0,1] and pointmass at 0</h3><strong>Nominal coverage probability 95.0%</strong>. <br /><strong>Estimated from 10000 replications.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mass at 0</th>\n",
       "      <th>sample size</th>\n",
       "      <th>Trunc Hoeff cov</th>\n",
       "      <th>MDKW cov</th>\n",
       "      <th>KW cov</th>\n",
       "      <th>Penny cov</th>\n",
       "      <th>Trunc Hoeff low</th>\n",
       "      <th>MDKW low</th>\n",
       "      <th>KW low</th>\n",
       "      <th>Penny low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900</td>\n",
       "      <td>25.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>99.86%</td>\n",
       "      <td>96.76%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.900</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>99.82%</td>\n",
       "      <td>96.47%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.900</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>99.97%</td>\n",
       "      <td>97.24%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.900</td>\n",
       "      <td>400.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>99.95%</td>\n",
       "      <td>95.53%</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990</td>\n",
       "      <td>25.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>99.88%</td>\n",
       "      <td>99.3%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.990</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>99.9%</td>\n",
       "      <td>97.43%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>99.91%</td>\n",
       "      <td>98.64%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.990</td>\n",
       "      <td>400.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>99.98%</td>\n",
       "      <td>98.24%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999</td>\n",
       "      <td>25.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>98.49%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.999</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>97.47%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>95.16%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999</td>\n",
       "      <td>400.0</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>98.17%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mass at 0  sample size Trunc Hoeff cov MDKW cov  KW cov Penny cov  \\\n",
       "0       0.900         25.0          100.0%   100.0%  99.86%    96.76%   \n",
       "1       0.900         50.0          100.0%   100.0%  99.82%    96.47%   \n",
       "2       0.900        100.0          100.0%   100.0%  99.97%    97.24%   \n",
       "3       0.900        400.0          100.0%   100.0%  99.95%    95.53%   \n",
       "4       0.990         25.0          100.0%   100.0%  99.88%     99.3%   \n",
       "5       0.990         50.0          100.0%   100.0%   99.9%    97.43%   \n",
       "6       0.990        100.0          100.0%   100.0%  99.91%    98.64%   \n",
       "7       0.990        400.0          100.0%   100.0%  99.98%    98.24%   \n",
       "8       0.999         25.0          100.0%   100.0%  100.0%    98.49%   \n",
       "9       0.999         50.0          100.0%   100.0%  100.0%    97.47%   \n",
       "10      0.999        100.0          100.0%   100.0%  100.0%    95.16%   \n",
       "11      0.999        400.0          100.0%   100.0%  100.0%    98.17%   \n",
       "\n",
       "   Trunc Hoeff low MDKW low  KW low Penny low  \n",
       "0              0.0   0.0001  0.0024    0.0094  \n",
       "1              0.0   0.0001  0.0023    0.0143  \n",
       "2              0.0   0.0005  0.0022     0.021  \n",
       "3           0.0001   0.0082  0.0023    0.0334  \n",
       "4              0.0      0.0     0.0    0.0003  \n",
       "5              0.0      0.0     0.0    0.0004  \n",
       "6              0.0      0.0     0.0    0.0006  \n",
       "7              0.0      0.0     0.0    0.0013  \n",
       "8              0.0      0.0     0.0       0.0  \n",
       "9              0.0      0.0     0.0       0.0  \n",
       "10             0.0      0.0     0.0       0.0  \n",
       "11             0.0      0.0     0.0       0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nonstandard mixture: a pointmass at zero and a uniform[0,1]\n",
    "ns = np.array([25, 50, 100, 400])  # sample sizes\n",
    "ps = np.array([0.9, 0.99, 0.999])    # mixture fraction, weight of pointmass\n",
    "alpha = 0.05  # 1- (confidence level)\n",
    "reps = int(1.0e4) # just for demonstration\n",
    "gamma = 0.99  # tuning constant in Kaplan-Wald\n",
    "xtol = 1.0e-6  # numerical tolerance for Kaplan-Wald\n",
    "\n",
    "cols = ['mass at 0', 'sample size', 'Trunc Hoeff cov', 'MDKW cov', 'KW cov', 'Penny cov',\\\n",
    "        'Trunc Hoeff low', 'MDKW low', 'KW low', 'Penny low']\n",
    "\n",
    "simTable = pd.DataFrame(columns=cols)\n",
    "\n",
    "for p in ps:\n",
    "    popMean = (1-p)*0.5  #  p*0 + (1-p)*.5\n",
    "    for n in ns:\n",
    "        hCrit = np.sqrt(-math.log(alpha/2)/(2*n))  # Hoeffding concentration bound\n",
    "        mCrit = np.sqrt(-np.log(alpha)/(2.0*n))  # the 1-sided MDKW constant\n",
    "        covH = 0\n",
    "        covM = 0\n",
    "        covK = 0\n",
    "        covP = 0\n",
    "        lowH = 0.0\n",
    "        lowM = 0.0\n",
    "        lowK = 0.0\n",
    "        lowP = 0.0\n",
    "\n",
    "        for rep in range(int(reps)):\n",
    "            sam = np.random.uniform(size=n)\n",
    "            ptMass = np.random.uniform(size=n)\n",
    "            pennies = np.random.uniform(size=n)\n",
    "            sam[ptMass < p] = 0.0\n",
    "            samMean = np.mean(sam)\n",
    "            #\n",
    "            hLow = max(samMean - hCrit, 0.0)\n",
    "            covH += (hLow <= popMean)\n",
    "            lowH += hLow\n",
    "            #\n",
    "            mLow = ksLowerMean(sam, mCrit)\n",
    "            covM += (mLow <= popMean)\n",
    "            lowM += mLow\n",
    "            #\n",
    "            kLow = kaplanWaldLowerCI(sam, cl = 1-alpha, gamma = 0.99, xtol = xtol)\n",
    "            covK += (kLow <= popMean)\n",
    "            lowK += kLow\n",
    "            #\n",
    "            pLow, s = pennyBinomialLowerBound(sam, np.r_[0:n], pennies, cl=1-alpha)\n",
    "            covP += (pLow <= popMean)\n",
    "            lowP += pLow\n",
    "            \n",
    "        simTable.loc[len(simTable)] =  p, n,\\\n",
    "            str(100*float(covH)/float(reps)) + '%',\\\n",
    "            str(100*float(covM)/float(reps)) + '%',\\\n",
    "            str(100*float(covK)/float(reps)) + '%',\\\n",
    "            str(100*float(covP)/float(reps)) + '%',\\\n",
    "            str(round(lowH/float(reps),4)),\\\n",
    "            str(round(lowM/float(reps),4)),\\\n",
    "            str(round(lowK/float(reps),4)),\\\n",
    "            str(round(lowP/float(reps), 4))\n",
    "#\n",
    "ansStr =  '<h3>Simulated coverage probability and expected lengths of one-sided nonparametric confidence intervals ' +\\\n",
    "          'mixture of U[0,1] and pointmass at 0</h3>' +\\\n",
    "          '<strong>Nominal coverage probability ' + str(100*(1-alpha)) +\\\n",
    "          '%</strong>. <br /><strong>Estimated from ' + str(int(reps)) + ' replications.</strong>'\n",
    "\n",
    "display(HTML(ansStr))\n",
    "display(simTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncated Hoeffding intervals do not appear to be competitive&mdash;despite the fact that they use more information than the Kaplan-Wald interval.  The Kaplan-Wald interval is slightly worse than the continuous penny sampling interval for this population (using this value of $\\gamma$), but KW requires only nonnegativity.\n",
    "\n",
    "Let's look at what happens with a pointmass at 1 instead of 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nonstandard mixture: a pointmass at 1 and a uniform[0,1]\n",
    "ns = np.array([25, 50, 100, 400])  # sample sizes\n",
    "ps = np.array([0.9, 0.99, 0.999])    # mixture fraction, weight of pointmass\n",
    "alpha = 0.05  # 1- (confidence level)\n",
    "reps = int(1.0e4) # just for demonstration\n",
    "gamma = 0.99  # tuning constant in Kaplan-Wald\n",
    "xtol = 1.0e-12\n",
    "\n",
    "cols = ['mass at 1', 'sample size', 'trunc Hoeff cov', 'MDKW cov', 'KW cov', 'Penny cov',\\\n",
    "        'trunc Hoeff low', 'MDKW low', 'KW low', 'Penny low']\n",
    "\n",
    "simTable = pd.DataFrame(columns=cols)\n",
    "\n",
    "for p in ps:\n",
    "    popMean = (1-p)*0.5 + p\n",
    "    for n in ns:\n",
    "        hCrit = np.sqrt(-math.log(alpha/2)/(2*n))  # Hoeffding concentration bound\n",
    "        mCrit = np.sqrt(-np.log(alpha)/(2.0*n))  # the 1-sided MDKW constant\n",
    "        covH = 0\n",
    "        covM = 0\n",
    "        covK = 0\n",
    "        covP = 0\n",
    "        lowH = 0.0\n",
    "        lowM = 0.0\n",
    "        lowK = 0.0\n",
    "        lowP = 0.0\n",
    "\n",
    "        for rep in range(int(reps)):\n",
    "            sam = np.random.uniform(size=n)\n",
    "            ptMass = np.random.uniform(size=n)\n",
    "            pennies = np.random.uniform(size=n)\n",
    "            sam[ptMass < p] = 1.0\n",
    "            samMean = np.mean(sam)\n",
    "            #\n",
    "            hLow = max(samMean - hCrit, 0.0)\n",
    "            covH += (hLow <= popMean)\n",
    "            lowH += hLow\n",
    "            #\n",
    "            mLow = ksLowerMean(sam, mCrit)\n",
    "            covM += (mLow <= popMean)\n",
    "            lowM += mLow\n",
    "            #\n",
    "            kLow = kaplanWaldLowerCI(sam, cl = 1-alpha, gamma = gamma, xtol = xtol)\n",
    "            covK += (kLow <= popMean)\n",
    "            lowK += kLow\n",
    "            #\n",
    "            pLow, s = pennyBinomialLowerBound(sam, np.r_[0:n], pennies, cl=1-alpha)\n",
    "            covP += (pLow <= popMean)\n",
    "            lowP += pLow\n",
    "            \n",
    "        simTable.loc[len(simTable)] =  p, n,\\\n",
    "            str(100*float(covH)/float(reps)) + '%',\\\n",
    "            str(100*float(covM)/float(reps)) + '%',\\\n",
    "            str(100*float(covK)/float(reps)) + '%',\\\n",
    "            str(100*float(covP)/float(reps)) + '%',\\\n",
    "            str(round(lowH/float(reps),4)),\\\n",
    "            str(round(lowM/float(reps),4)),\\\n",
    "            str(round(lowK/float(reps),4)),\\\n",
    "            str(round(lowP/float(reps),4))\n",
    "#\n",
    "ansStr =  '<h3>Simulated coverage probability and expected lengths of one-sided nonparametric confidence intervals ' +\\\n",
    "          'mixture of U[0,1] and pointmass at 1</h3>' +\\\n",
    "          '<strong>Nominal coverage probability ' + str(100*(1-alpha)) +\\\n",
    "          '%</strong>. <br /><strong>Estimated from ' + str(int(reps)) + ' replications.</strong>'\n",
    "\n",
    "display(HTML(ansStr))\n",
    "display(simTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, the Kaplan-Wald method performs essentially the same as Continuous Penny Sampling (with $\\gamma = 0.99$), even though KW only requires nonnegativity, and Continuous Penny Sampling requires an upper bound on the population as well.\n",
    "\n",
    "Let's see what happens as $\\gamma$ varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nonstandard mixture: a pointmass at 0 and a uniform[0,1]\n",
    "ns = np.array([25, 50, 100, 400])  # sample sizes\n",
    "ps = np.array([0.9, 0.99, 0.999])    # mixture fraction, weight of pointmass\n",
    "alpha = 0.05  # 1- (confidence level)\n",
    "reps = int(1.0e4) # just for demonstration\n",
    "gamma = np.array([0.01, 0.1, 0.5, 0.9, 0.999])  # tuning constant in Kaplan-Wald\n",
    "xtol = 1.0e-12\n",
    "\n",
    "cols = ['mass at 0', 'sample size']\n",
    "cols.extend(['KW cov ' + str(g) for g in gamma])\n",
    "cols.extend(['KW low ' + str(g) for g in gamma])\n",
    "\n",
    "\n",
    "simTable = pd.DataFrame(columns=cols)\n",
    "\n",
    "for p in ps:\n",
    "    popMean = (1-p)*0.5\n",
    "    for n in ns:\n",
    "        covK = np.zeros(len(gamma))\n",
    "        lowK = np.zeros(len(gamma))\n",
    "\n",
    "        for rep in range(int(reps)):\n",
    "            sam = np.random.uniform(size=n)\n",
    "            ptMass = np.random.uniform(size=n)\n",
    "            pennies = np.random.uniform(size=n)\n",
    "            sam[ptMass < p] = 0.0\n",
    "            samMean = np.mean(sam)\n",
    "            #\n",
    "            for i in range(len(gamma)):\n",
    "                kLow = kaplanWaldLowerCI(sam, cl = 1-alpha, gamma = gamma[i], xtol = xtol)\n",
    "                covK[i] += (kLow <= popMean)\n",
    "                lowK[i] += kLow\n",
    "            #\n",
    "            \n",
    "        theRow = [p, n]\n",
    "        theRow.extend([str(100*float(covK[i])/float(reps)) + '%' for i in range(len(gamma))])\n",
    "        theRow.extend([str(round(lowK[i]/float(reps),4)) for i in range(len(gamma))])\n",
    "        simTable.loc[len(simTable)] = theRow\n",
    "#\n",
    "ansStr =  '<h3>Simulated coverage probability and expected lengths of one-sided nonparametric confidence intervals ' +\\\n",
    "          'mixture of U[0,1] and pointmass at 0</h3>' +\\\n",
    "          '<strong>Nominal coverage probability ' + str(100*(1-alpha)) +\\\n",
    "          '%</strong>. <br /><strong>Estimated from ' + str(int(reps)) + ' replications.</strong>'\n",
    "\n",
    "display(HTML(ansStr))\n",
    "display(simTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, smaller values of $\\gamma$ improve the confidence bound when many observations are (nearly) zero. The Kaplan-Wald method is quite competitive with Continuous Penny Sampling in this case when $\\gamma = 0.1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nonstandard mixture: a pointmass at 1 and a uniform[0,1]\n",
    "ns = np.array([25, 50, 100, 400])  # sample sizes\n",
    "ps = np.array([0.9, 0.99, 0.999])    # mixture fraction, weight of pointmass\n",
    "alpha = 0.05  # 1- (confidence level)\n",
    "reps = int(1.0e4) # just for demonstration\n",
    "gamma = np.array([0.01, 0.1, 0.5, 0.9, 0.999])  # tuning constant in Kaplan-Wald\n",
    "xtol = 1.0e-12\n",
    "\n",
    "cols = ['mass at 1', 'sample size']\n",
    "cols.extend(['KW cov ' + str(g) for g in gamma])\n",
    "cols.extend(['KW low ' + str(g) for g in gamma])\n",
    "\n",
    "simTable = pd.DataFrame(columns=cols)\n",
    "\n",
    "for p in ps:\n",
    "    popMean = (1-p)*0.5 + p\n",
    "    for n in ns:\n",
    "        covK = np.zeros(len(gamma))\n",
    "        lowK = np.zeros(len(gamma))\n",
    "\n",
    "        for rep in range(int(reps)):\n",
    "            sam = np.random.uniform(size=n)\n",
    "            ptMass = np.random.uniform(size=n)\n",
    "            pennies = np.random.uniform(size=n)\n",
    "            sam[ptMass < p] = 1.0\n",
    "            samMean = np.mean(sam)\n",
    "            #\n",
    "            for i in range(len(gamma)):\n",
    "                kLow = kaplanWaldLowerCI(sam, cl = 1-alpha, gamma = gamma[i], xtol = xtol)\n",
    "                covK[i] += (kLow <= popMean)\n",
    "                lowK[i] += kLow\n",
    "            #\n",
    "            \n",
    "        theRow = [p, n]\n",
    "        theRow.extend([str(100*float(covK[i])/float(reps)) + '%' for i in range(len(gamma))])\n",
    "        theRow.extend([str(round(lowK[i]/float(reps),4)) for i in range(len(gamma))])\n",
    "        simTable.loc[len(simTable)] = theRow\n",
    "#\n",
    "ansStr =  '<h3>Simulated coverage probability and expected lengths of one-sided nonparametric confidence intervals ' +\\\n",
    "          'mixture of U[0,1] and pointmass at 1</h3>' +\\\n",
    "          '<strong>Nominal coverage probability ' + str(100*(1-alpha)) +\\\n",
    "          '%</strong>. <br /><strong>Estimated from ' + str(int(reps)) + ' replications.</strong>'\n",
    "\n",
    "display(HTML(ansStr))\n",
    "display(simTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using a small value of $\\gamma$ hurts the confidence bound&mdash;at least for small sample sizes&mdash;when $x$ tends to have *few* values near zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "+ [Previous: Penny Sampling](pennySampling.ipynb)\n",
    "+ [Index](index.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run talkTools.py"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
